"""
Step 1: News Fetcher
Fetches news from all 4 sources (Moneycontrol, LiveMint, CNBC-TV18, Economic Times)
Saves per-source: all_*.json (permanent/append) and *_new.json (current run only)
Creates merged_news.json with all sources combined
"""
import os
import json
from datetime import datetime
import concurrent.futures

from config import (
    MAX_ARTICLES,
    LOG_FILE,
    NEWS_FETCHER_OUTPUT_DIR,
    MONEYCONTROL_ALL_PATH,
    MONEYCONTROL_NEW_PATH,
    LIVEMINT_ALL_PATH,
    LIVEMINT_NEW_PATH,
    CNBC_ALL_PATH,
    CNBC_NEW_PATH,
    ET_ALL_PATH,
    ET_NEW_PATH,
    BUSINESS_TODAY_ALL_PATH,
    BUSINESS_TODAY_NEW_PATH,
    HINDU_BL_ALL_PATH,
    HINDU_BL_NEW_PATH,
    MERGED_NEWS_ALL_PATH,
    MERGED_NEWS_NEW_PATH,
)

from modules.news_sources.moneycontrol import pull as pull_moneycontrol
from modules.news_sources.livemint import pull as pull_livemint
from modules.news_sources.the_economic_times import fetch_and_save_articles as fetch_et
from modules.news_sources.cnbc_tv18 import fetch_and_save_articles as fetch_cnbc
from modules.news_sources.business_today import collect_candidate_links, extract_content_and_time, extract_article_id as bt_extract_id
from modules.news_sources.hindu_business_line import fetch_bl_headlines, fetch_full_bl_article, extract_bl_article_id


def fetch_business_today(max_articles: int = 12) -> list:
    """Fetch articles from Business Today and return in standard format."""
    candidates = collect_candidate_links(max_links=max_articles)
    if not candidates:
        return []
    
    results = []
    for c in candidates:
        content, published = extract_content_and_time(c["url"])
        if not content:
            continue
        aid = bt_extract_id(c["url"])
        results.append({
            "article_id": aid,
            "headline": c["headline"],
            "content": content,
            "url": c["url"],
            "published_time": published,
            "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source": "BusinessToday"
        })
    return results


def fetch_hindu_business_line(max_articles: int = 10) -> list:
    """Fetch articles from Hindu Business Line and return in standard format."""
    articles = fetch_bl_headlines(max_articles=max_articles)
    if not articles:
        return []
    
    results = []
    for article in articles:
        content, published_time = fetch_full_bl_article(article["url"])
        if not content:
            continue
        article_id = extract_bl_article_id(article["url"])
        results.append({
            "article_id": article_id,
            "headline": article["headline"],
            "content": content,
            "url": article["url"],
            "published_time": published_time,
            "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source": "The Hindu Business Line"
        })
    return results


def process_business_today(max_articles: int) -> list:
    """Wrapper to fetch and save Business Today articles."""
    log("â–¶ Fetching Business Today...")
    try:
        bt_articles = fetch_business_today(max_articles=max_articles)
        if bt_articles:
            fresh_bt = append_to_all(BUSINESS_TODAY_ALL_PATH, bt_articles)
            save_json(BUSINESS_TODAY_NEW_PATH, fresh_bt)
            log(f"âœ” Business Today: {len(fresh_bt)} new articles")
            return fresh_bt
        else:
            log("ðŸŸ¡ Business Today: No articles fetched")
            save_json(BUSINESS_TODAY_NEW_PATH, [])
            return []
    except Exception as e:
        log(f"âœ– Business Today failed: {e}")
        save_json(BUSINESS_TODAY_NEW_PATH, [])
        return []


def process_hindu_business_line(max_articles: int) -> list:
    """Wrapper to fetch and save Hindu Business Line articles."""
    log("â–¶ Fetching Hindu Business Line...")
    try:
        hbl_articles = fetch_hindu_business_line(max_articles=max_articles)
        if hbl_articles:
            fresh_hbl = append_to_all(HINDU_BL_ALL_PATH, hbl_articles)
            save_json(HINDU_BL_NEW_PATH, fresh_hbl)
            log(f"âœ” Hindu Business Line: {len(fresh_hbl)} new articles")
            return fresh_hbl
        else:
            log("ðŸŸ¡ Hindu Business Line: No articles fetched")
            save_json(HINDU_BL_NEW_PATH, [])
            return []
    except Exception as e:
        log(f"âœ– Hindu Business Line failed: {e}")
        save_json(HINDU_BL_NEW_PATH, [])
        return []


def log(msg: str):
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"{datetime.now()} | [news_fetcher_step1] {msg}\n")
    print(f"[news_fetcher_step1] {msg}")


def load_json(path: str) -> list:
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            log(f"JSON load error ({path}): {e}")
    return []


def save_json(path: str, data: list):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def dedup_articles(articles: list) -> list:
    """Deduplicate by article_id"""
    seen = set()
    out = []
    for a in articles:
        aid = a.get("article_id")
        if aid in seen:
            continue
        seen.add(aid)
        out.append(a)
    return out


def append_to_all(all_path: str, new_articles: list) -> list:
    """
    Append new_articles to all_path, deduplicating by article_id.
    Returns the list of truly new articles that were appended.
    """
    existing = load_json(all_path)
    existing_ids = {a.get("article_id") for a in existing}
    
    fresh = [a for a in new_articles if a.get("article_id") not in existing_ids]
    
    if fresh:
        all_data = existing + fresh
        save_json(all_path, all_data)
        log(f"ðŸ’¾ Appended {len(fresh)} new to {all_path} (total: {len(all_data)})")
    else:
        log(f"ðŸŸ¡ No new articles to append to {all_path}")
    
    return fresh


def fetch_source(name: str, fetch_fn, all_path: str, new_path: str, max_articles: int) -> list:
    """
    Fetch from a single source, save to all_*.json and *_new.json
    Returns the list of new articles from this run.
    """
    log(f"â–¶ Fetching {name}...")
    
    try:
        if name in ["Moneycontrol", "LiveMint"]:
            articles = fetch_fn(max_articles=max_articles)
        else:
            articles = fetch_fn(max_articles=max_articles)
        
        if not articles:
            log(f"ðŸŸ¡ {name}: No articles fetched")
            save_json(new_path, [])
            return []
        
        log(f"âœ” {name}: Fetched {len(articles)} articles")
        
        fresh = append_to_all(all_path, articles)
        save_json(new_path, fresh)
        log(f"ðŸ†• {name}: Wrote {len(fresh)} new articles to {new_path}")
        
        return fresh
        
    except Exception as e:
        log(f"âœ– {name} failed: {e}")
        save_json(new_path, [])
        return []


def run_news_fetcher(max_articles: int = MAX_ARTICLES) -> list:
    """
    Main entry point for Step 1.
    Fetches from all sources in PARALLEL, saves per-source files, creates merged_news.json
    Returns the list of all new articles from this run.
    """
    log("=" * 60)
    log("Step 1: News Fetcher Started (Parallel Mode)")
    log("=" * 60)
    
    os.makedirs(NEWS_FETCHER_OUTPUT_DIR, exist_ok=True)
    
    all_new = []
    
    # Define tasks for parallel execution
    # Each task is a tuple: (function, name, args_dict)
    tasks = [
        # Moneycontrol
        (fetch_source, "Moneycontrol", {
            "name": "Moneycontrol", "fetch_fn": pull_moneycontrol, 
            "all_path": MONEYCONTROL_ALL_PATH, "new_path": MONEYCONTROL_NEW_PATH, 
            "max_articles": max_articles
        }),
        # LiveMint
        (fetch_source, "LiveMint", {
            "name": "LiveMint", "fetch_fn": pull_livemint, 
            "all_path": LIVEMINT_ALL_PATH, "new_path": LIVEMINT_NEW_PATH, 
            "max_articles": max_articles
        }),
        # Economic Times
        (fetch_et, "Economic Times", {"max_articles": max_articles}),
        # CNBC-TV18
        (fetch_cnbc, "CNBC-TV18", {"max_articles": max_articles}),
        # Business Today
        (process_business_today, "Business Today", {"max_articles": max_articles}),
        # Hindu Business Line
        (process_hindu_business_line, "Hindu Business Line", {"max_articles": max_articles})
    ]
    
    # Run tasks in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
        future_map = {
            executor.submit(func, **kwargs): name 
            for func, name, kwargs in tasks
        }
        
        for future in concurrent.futures.as_completed(future_map):
            name = future_map[future]
            try:
                result = future.result()
                if result:
                    all_new.extend(result)
            except Exception as e:
                log(f"âœ– Task failed for {name}: {e}")

    # Deduplicate merged
    merged = dedup_articles(all_new)
    if len(merged) != len(all_new):
        log(f"ðŸŸ¡ Global dedup removed {len(all_new) - len(merged)} duplicate(s)")
    
    # Save news_new.json (current run only)
    save_json(MERGED_NEWS_NEW_PATH, merged)
    log(f"ðŸ’¾ Saved news_new.json ({len(merged)} articles)")
    
    # Append to all_news.json (cumulative)
    truly_new = append_to_all(MERGED_NEWS_ALL_PATH, merged)
    log(f"âž• Appended {len(truly_new)} to all_news.json (cumulative)")
    
    log("=" * 60)
    log(f"Step 1 Complete: {len(merged)} new articles fetched")
    log("=" * 60)
    
    return merged


if __name__ == "__main__":
    articles = run_news_fetcher()
    print(f"\nâœ… News Fetcher completed: {len(articles)} new articles")
